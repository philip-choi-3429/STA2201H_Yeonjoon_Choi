---
title: "Week 5 Lab Stat2201"
author: "Yeonjoon Choi"
date: "2023-02-10"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Today we will be starting off using Stan, looking at the kid's test score data set (available in resources for the [Gelman Hill textbook](https://mc-stan.org/rstanarm/reference/rstanarm-datasets.html)). 

```{r, message = FALSE, warning=FALSE}
library(tidyverse)
library(rstan)
library(tidybayes)
library(here)
```


The data look like this:

```{r, message = FALSE, echo = FALSE, warning = FALSE}
require(curl)
gurl = "https://raw.githubusercontent.com/MJAlexander/applied-stats-2023/main/data/kidiq.RDS"
kidiq = readRDS(url(gurl, method="libcurl"))
kidiq

```
As well as the kid's test scores, we have a binary variable indicating whether or not the mother completed high school, the mother's IQ and age. 


# Descriptives

## Question 1

Use plots or tables to show three interesting observations about the data. Remember:

- Explain what your graph/ tables show
- Choose a graph type that's appropriate to the data type

```{r, echo = FALSE}
ggplot(data = kidiq, aes(color = as.factor(mom_hs), y = kid_score, x = mom_iq))+
  geom_point()+
  geom_smooth(method="lm", formula = y~x)+
  xlab("IQ of Mother")+
  ylab("Score of the Kid")+
  labs(title = "Kid Score vs Mom IQ", color = "Mother High School")+
  scale_color_manual(labels = c("Did not Graduate High School", "Graduated High School"),
                     values = c("blue", "red"))+
  theme_bw()

```

We see in general that higher IQ score from the mother corresponded to higher test score from the kid. We see that IQ of the mother had a bigger positive effect on the score of the children for mothers who did not graduate from high school.


```{r, echo = FALSE, message= FALSE, warning=FALSE}
temp = kidiq|>
  select(c(1,2))

ggplot(temp, aes(x = kid_score)) +
  geom_histogram(aes(y = ..density..), fill = "cyan", color = "blue",
                 data = ~ subset(., mom_hs == 0)) +
  geom_label(aes(x=50, y=0.04, label="Did not Graduate High School"), color="black") +
  geom_histogram(aes(y = -..density..), fill = "coral1", color = "red",
                 data = ~ subset(., mom_hs == 1)) +
  geom_label(aes(x=50, y=-0.04, label="Graduated High School"), color="black") +
  geom_hline(yintercept = 0)+
  xlab("Kid Test Score")+
  labs(title = "Density Histogram of Kid Test Score")+
  theme_bw()
```

Comparing the two density histograms for did not graduate high school group and graduated high school group, we see that lower test score had higher density in did not graduate high school group (Compare the two group for test score of less than $50$, and the difference is clear). And the graduated high school group has higher density for test scores greater than $100$. In general, children with mother who graduated high school has higher test scores. 


```{r, echo = FALSE}
ggplot(kidiq, aes(x= as.factor(mom_age), y = kid_score))+
  geom_boxplot()+
  geom_point(alpha = 0.5, aes(color = as.factor(mom_hs)))+
  xlab("Age of the Mother")+
  ylab("Kid Test Score")+
  labs(title = "Age of the Mother and Kid Test Score")+
  scale_color_manual(labels = c("Did not Graduate High School", "Graduated High School"),
                     values = c("red", "blue"), name = "High School")+
  theme_bw()


```

From the boxplot, there does not seem to be any relationship between age of the mother and the test score of the kids. 

We may expect younger mother may be more likely to have not graduated from high school since unexpected teen pregnancy can hinder education. However, from the plot, such a pattern does not emerge. Majority of mothers aged 17 and 18 did not graduate from high school, but the sample size for that age group is small to make any conclusion. 
# Estimating mean, no covariates

In class we were trying to estimate the mean and standard deviation of the kid's test scores. The `kids2.stan` file contains a Stan model to do this. If you look at it, you will notice the first `data` chunk lists some inputs that we have to define: the outcome variable `y`, number of observations `N`, and the mean and standard deviation of the prior on `mu`. Let's define all these values in a `data` list.


```{r}
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 10
# named list to input for stan function
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)
```



Now we can run the model. Please change the file directory as needed. You can see where my stan file is stored on my Github. 

```{r, message = FALSE, warning = FALSE}

fit = stan(file = here("kids2.stan"),
            data = data,
            chains = 3,
            iter = 500, refresh = 0)

```

Look at the summary

```{r}
fit
```

Traceplot

```{r}
traceplot(fit)
```

All looks fine. 

```{r, message = FALSE, warning = FALSE}
pairs(fit, pars = c("mu", "sigma"))
```

```{r}
stan_dens(fit, separate_chains = TRUE)
```


## Understanding output

What does the model actually give us? A number of samples from the posteriors. To see this, we can use `extract` to get the samples. 

```{r}
post_samples <- extract(fit)
head(post_samples[["mu"]])
```


This is a list, and in this case, each element of the list has 4000 samples. E.g. quickly plot a histogram of mu

```{r}
hist(post_samples[["mu"]])
median(post_samples[["mu"]])
# 95% bayesian credible interval
quantile(post_samples[["mu"]], 0.025)
quantile(post_samples[["mu"]], 0.975)
```



## Plot estimates

There are a bunch of packages, built-in functions that let you plot the estimates from the model, and I encourage you to explore these options (particularly in `bayesplot`, which we will most likely be using later on). I like using the `tidybayes` package, which allows us to easily get the posterior samples in a tidy format (e.g. using gather draws to get in long format). Once we have that, it's easy to just pipe and do ggplots as usual. 


Get the posterior samples for mu and sigma in long format:

```{r}
dsamples <- fit  |> 
  gather_draws(mu, sigma) # gather = long format
dsamples
# wide format
fit  |>  spread_draws(mu, sigma)
# quickly calculate the quantiles using 
dsamples |> 
  median_qi(.width = 0.8)
```

Let's plot the density of the posterior samples for mu and add in the prior distribution

```{r, warning = FALSE, message = FALSE, echo = FALSE}
dsamples |> 
  filter(.variable == "mu") |> 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(70, 100)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mean test scores") + 
  xlab("score")
  
```

## Question 2

Change the prior to be much more informative (by changing the standard deviation to be 0.1). Rerun the model. Do the estimates change? Plot the prior and posterior densities. 


```{r, warning = FALSE, message = FALSE, echo = FALSE}
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 0.1
# named list to input for stan function
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)

fit_new = stan(file = here("kids2.stan"),
            data = data,
            chains = 3,
            iter = 500, refresh = 0)

```


The new model is the following: 

```{r}
fit_new
```
```{r}
post_samples = extract(fit_new)

hist(post_samples[["mu"]])
median(post_samples[["mu"]])
# 95% bayesian credible interval
quantile(post_samples[["mu"]], 0.025)
quantile(post_samples[["mu"]], 0.975)
```

We can see that since our prior was very informative, even after seeing the data, our point estimate is still close to $80$ compared to the model with less informative prior. The estimate for $\sigma^2$ did not change much as $\sigma_0^2$ is not related to the prior for $\sigma_0^2$. However, we see that our point estimate for $\mu$ is now approximately $80$, when before the point estimate was approximate $87$. 


```{r, warning = FALSE, message = FALSE, echo = FALSE}
dsamples = fit_new  |> 
  gather_draws(mu, sigma) # gather = long format


dsamples |> 
  filter(.variable == "mu") |> 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(70, 100)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mean test scores") + 
  xlab("score")

```

We see that the posterior distribution did not change much compared to the prior distribution. 

# Adding covariates

Now let's see how kid's test scores are related to mother's education. We want to run the simple linear regression

$$
Score = \alpha + \beta X
$$
where $X = 1$ if the mother finished high school and zero otherwise. 

`kid3.stan` has the stan model to do this. Notice now we have some inputs related to the design matrix $X$ and the number of covariates (in this case, it's just 1).

Let's get the data we need and run the model. 



```{r, message = FALSE, warning = FALSE}
X <- as.matrix(kidiq$mom_hs, ncol = 1) # force this to be a matrix
K <- 1
data <- list(y = y, N = length(y), 
             X =X, K = K)
fit2 <- stan(file = here("kids3.stan"),
            data = data, 
            iter = 1000, refresh = 0)
```

## Question 3

a) Confirm that the estimates of the intercept and slope are comparable to results from `lm()` 

```{r}
lm_model = lm(kid_score~as.factor(mom_hs), data = kidiq)
summary(lm_model)

fit2
```

We do see that the point estimate of the intercept and the $\beta_1$ are very similar, as well as the estimate for $\sigma$, since residual standard error is the standard point estimate for $\sigma$ in the framework of linear regression.


```{r}
post_samples = extract(fit2)

quantile(post_samples[["beta"]], 0.025)
quantile(post_samples[["beta"]], 0.975)

quantile(post_samples[["alpha"]], 0.025)
quantile(post_samples[["alpha"]], 0.975)


confint(lm_model)


```
Even though the interpretation is drastically different, the credible interval for $\alpha$ and $\beta$ and the confidence interval for the two parameters are also similar. 

b) Do a `pairs` plot to investigate the joint sample distributions of the slope and intercept. Comment briefly on what you see. Is this potentially a problem?


```{r, warning=FALSE}
pairs(fit2, pars = c("alpha", "beta"))
```


We see that in the joint distribution there are some strong correlations between the slope and the intercept. In particular, if the slope is small, the intercept is big, and if the slope is big, the intercept is small. 

The potential problem here is that this will make the posterior sampling possibly inefficient. 

## Plotting results

It might be nice to plot the posterior samples of the estimates for the non-high-school and high-school mothered kids. Here's some code that does this: notice the `beta[condition]` syntax. Also notice I'm using `spread_draws`, because it's easier to calculate the estimated effects in wide format

```{r, warning=FALSE, message = FALSE}
fit2 |>
  spread_draws(alpha, beta[k], sigma) |> 
     mutate(nhs = alpha, # no high school is just the intercept
          hs = alpha + beta) |> 
  select(nhs, hs) |> 
  pivot_longer(nhs:hs, names_to = "education", values_to = "estimated_score") |> 
  ggplot(aes(y = education, x = estimated_score)) +
  stat_halfeye() + 
  theme_bw() + 
  ggtitle("Posterior estimates of scores by education level of mother")
  
```


## Question 4

Add in mother's IQ as a covariate and rerun the model. Please  mean center the covariate before putting it into the model. Interpret the coefficient on the (centered) mum's IQ. 


```{r, message = FALSE, warning = FALSE, echo = FALSE}
X = as.matrix(cbind(kidiq$mom_hs, scale(kidiq$mom_iq))) #Scaled the IQ
K = 2
data = list(y = y, N = length(y), 
             X=X, K = K)
fit3 = stan(file = here("kids3.stan"),
            data = data, 
            iter = 1000, refresh = 0)
```


The returned model is the following.

```{r}
fit3
```

We will use the mean of $8.41$ as the point estimate for the coefficient on the IQ of the mother. The interpretation is that increase corresponding to one standard deviation(which is 15 in this case) results in increase of $8.41$ in the expected score of the children. 

## Question 5 

Confirm the results from Stan agree with `lm()`

```{r}
lm_model = lm(kid_score~as.factor(mom_hs)+scale(mom_iq), data = kidiq)

summary(lm_model)
```

We can see that point estimate for the intercept, $\beta_1$, and $\beta_2$, and $\sigma$ are all very similar to that of the results from Stan. 

```{r}
confint(lm_model)

post_samples = extract(fit3)

#beta_3
quantile(post_samples[["beta"]][,1], 0.025)
quantile(post_samples[["beta"]][,1], 0.975)


#beta_2
quantile(post_samples[["beta"]][,2], 0.025)
quantile(post_samples[["beta"]][,2], 0.975)
```

Even though the interpretation is different, we also see that the $95 \%$ from the linear model and the $95 \%$ credible intervals are also similar. 

## Question 6

Plot the posterior estimates of scores by education of mother for mothers who have an IQ of 110. 

```{r, echo = FALSE}
high_school = post_samples[["beta"]][,1]+((110-mean(kidiq$mom_iq))/sd(kidiq$mom_iq))*post_samples[["beta"]][,2]+post_samples[["alpha"]]


no_high = ((110-mean(kidiq$mom_iq))/sd(kidiq$mom_iq))*post_samples[["beta"]][,2]+
  post_samples[["alpha"]]

df = data.frame('High School' = c(rep("Graduated High School",2000), 
                                  rep("Did not Graduate High School", 2000)), 
                estimate = c(high_school, no_high))


ggplot(df, aes(x = estimate, fill = High.School))+
  geom_histogram(alpha=0.5, position="dodge", binwidth = 0.32) + 
  xlab("Posterior Estimates of Kid Test Score")+
  scale_fill_manual(labels = c("Did not Graduate High School", "Graduated High School"),
                     values = c("blue", "red"), name = "High School")+
  labs(title = "Posterior Estimates of Kid Test Score for Mother with an IQ of 110")+
  theme_bw()
  
```

## Question 7

Generate and plot (as a histogram) samples from the posterior predictive distribution for a new kid with a mother who graduated high school and has an IQ of 95. 


```{r, echo = FALSE, warning = FALSE, message = FALSE}
sigma = post_samples[["sigma"]]
high_school = post_samples[["beta"]][,1]+((95-mean(kidiq$mom_iq))/sd(kidiq$mom_iq))*post_samples[["beta"]][,2]+post_samples[["alpha"]]

set.seed(19823)
high_school_samp = rnorm(n = length(sigma), mean = high_school, sd = sigma)


ggplot(data.frame(high_school_samp), aes(x = high_school_samp))+
  geom_histogram()+
  xlab("Kid Test Score")+
  labs(title = "Posterior Preditive Distribution of Kid with Mother of IQ 95 who is a High School Graduate")+
  theme_bw()
```


For question 6, it was not clear to me if the question wanted posterior predictive distribution or not. Below, I plot the posterior predictive distribution for question 6.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
high_school = post_samples[["beta"]][,1]+((110-mean(kidiq$mom_iq))/sd(kidiq$mom_iq))*post_samples[["beta"]][,2]+post_samples[["alpha"]]


no_high = ((110-mean(kidiq$mom_iq))/sd(kidiq$mom_iq))*post_samples[["beta"]][,2]+post_samples[["alpha"]]


set.seed(93921)
df = data.frame('High School' = c(rep("Graduated High School",2000), rep("Did not Graduate High School", 2000)), estimate = c(rnorm(n = length(sigma), mean = high_school, sd = sigma), rnorm(n = length(sigma), no_high, sd = sigma)))


ggplot(df, aes(x = estimate, fill = High.School))+
  geom_histogram(alpha=0.5, position="dodge", binwidth = 5) + 
  xlab("Posterior Estimates of Kid Test Score")+
  scale_fill_manual(labels = c("Did not Graduate High School", "Graduated High School"),
                     values = c("blue", "red"), name = "High School")+
  labs(title = "Posterior Estimates of Kid Test Score for Mother with an IQ of 110")+
  theme_bw()

```

With the added uncertainty, the distribution is not as separated compared to what we did first in question 6. 